{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline.tok2vec import DEFAULT_TOK2VEC_MODEL\n",
    "from spacy.training.example import Example\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('dataset_agosto2024.csv')\n",
    "geolocator = Nominatim(user_agent=\"Sophia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo de spaCy para español\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = train_df[\"clase\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtén 1000 filas aleatorias del DataFrame\n",
    "n_datos = len(train_df)#Se estan tomando todas las filas\n",
    "random_rows = train_df.sample(n=n_datos, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_use = \"title\" #Puede ser title o text\n",
    "\n",
    "# Preparar los datos para el entrenamiento\n",
    "train_data = [\n",
    "    (row[text_to_use], {\"cats\": {cls: (cls == row['clase']) for cls in unique_classes}})\n",
    "    for _, row in random_rows.iterrows()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "\n",
    "for example in train_data:\n",
    "    train_examples.append(Example.from_dict(nlp.make_doc(example[0]), example[1]))\n",
    "\n",
    "def get_examples():\n",
    "    return train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "            \"@architectures\": \"spacy.TextCatCNN.v2\",\n",
    "            \"exclusive_classes\": True,\n",
    "            \"tok2vec\": DEFAULT_TOK2VEC_MODEL,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the textcat component to the pipeline\n",
    "textcat = nlp.add_pipe(\"textcat\", config={\"model\": model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcat.initialize(get_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'textcat': 154.31192181259394}\n",
      "{'textcat': 145.2422415278852}\n",
      "{'textcat': 139.06601426377892}\n",
      "{'textcat': 134.36419315449893}\n",
      "{'textcat': 130.19400797970593}\n",
      "{'textcat': 127.0931301843375}\n",
      "{'textcat': 124.85290009621531}\n",
      "{'textcat': 121.09695628285408}\n",
      "{'textcat': 119.61362112534698}\n",
      "{'textcat': 117.42926041875035}\n"
     ]
    }
   ],
   "source": [
    "iteraciones = 10\n",
    "\n",
    "# Entrenar el modelo\n",
    "with nlp.select_pipes(enable=\"textcat\"):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for epoch in range(iteraciones):\n",
    "        losses = {}\n",
    "        random.shuffle(train_data)\n",
    "        # Dividir los datos en lotes y actualizar el modelo\n",
    "        for batch in minibatch(train_data, size=compounding(4.0, 32.0, 1.001)):\n",
    "            texts, annotations = zip(*batch)\n",
    "            example = []\n",
    "            # Actualizar el modelo con iteraciones\n",
    "            for i in range(len(texts)):\n",
    "                doc = nlp.make_doc(texts[i])\n",
    "                example.append(Example.from_dict(doc, annotations[i]))\n",
    "            nlp.update(example, drop=0.5, losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado en disco\n",
    "nlp.to_disk(\"model_1000_CNNv2\")\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "nlp_loaded = spacy.load(\"model_1000_CNNv2\")#modelo_clasificador_noticias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificar las noticias del conjunto de prueba\n",
    "random_test_data = test_df.sample(n=100, random_state=42)\n",
    "# test_data = random_test_data[\"title\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "comunas = pd.read_csv(\"comunas.csv\", sep=\";\")\n",
    "comunaslist = comunas[\"Nombre\"].values\n",
    "paises = pd.read_csv(\"paises.csv\")\n",
    "paisesList = paises[\"nombre\"].values\n",
    "strings_limpios = [s.strip(' \"\\'') for s in paisesList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para clasificar texto\n",
    "def classify_text(text):\n",
    "    doc = nlp(text)\n",
    "    scores = doc.cats\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# Función para extraer el evento principal\n",
    "def extract_event(doc):\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            event = token.text\n",
    "            for child in token.children:\n",
    "                if child.dep_ in [\"nsubj\", \"dobj\"]:\n",
    "                    event = f\"{child.text} {event}\"\n",
    "            return event\n",
    "    return \"No se pudo extraer el evento\"\n",
    "\n",
    "# Función para extraer la dirección\n",
    "def extract_address(doc):\n",
    "    # Variables para almacenar diferentes tipos de locaciones\n",
    "    direcciones = []\n",
    "    comunas_encontradas = []\n",
    "    paises_encontrados = []\n",
    "\n",
    "    # Extraer entidades de tipo GPE\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"LOC\", \"GPE\"] :\n",
    "            # Clasificar en comuna, país o dirección\n",
    "            if ent.text in comunaslist:\n",
    "                comunas_encontradas.append(ent.text)\n",
    "            elif ent.text in strings_limpios:\n",
    "                paises_encontrados.append(ent.text)\n",
    "            else:\n",
    "                direcciones.append(ent.text)\n",
    "    responseText = \"\"\n",
    "    if len(direcciones) > 0:\n",
    "        responseText += direcciones[0]\n",
    "    if len(comunas_encontradas) > 0:\n",
    "        responseText += \", \" + comunas_encontradas[0]\n",
    "    if len(paises_encontrados) > 0:\n",
    "        responseText += \", \" + paises_encontrados[0] \n",
    "    else: responseText += \", Chile\"\n",
    "    return responseText\n",
    "\n",
    "# Configurar el geocodificador\n",
    "geolocator = Nominatim(user_agent=\"my_agent\")\n",
    "\n",
    "# Función para geocodificar una dirección\n",
    "def geocode_address(address):\n",
    "    try:\n",
    "        location = geolocator.geocode(address)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for _, row in random_test_data.iterrows():\n",
    "    doc = nlp(row['text'])\n",
    "    event = extract_event(doc)\n",
    "    category = classify_text(row['text'])\n",
    "    address = extract_address(doc)\n",
    "    lat, lon = geocode_address(address)\n",
    "    \n",
    "    results.append({\n",
    "        'id_news': row['id_news'],\n",
    "        'event': event,\n",
    "        'category': category,\n",
    "        'address': address,\n",
    "        'latitud': lat,\n",
    "        'longitud': lon\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('output_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
